{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will unveil the basic mechanism of the computational process of BigDL using a simple example. In this example, we show that how to obtain the gradients for updating with a single forward and backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a simple linear regression which can be formulized as *y = Wx + b*ï¼Œ where *W = [w1,w2]* are weight parameters and *b* is the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createLinear\n",
      "{u'com.intel.analytics.bigdl.nn.Linear@a835bb1f': {u'gradWeight': array([[ 0.,  0.]], dtype=float32), u'bias': array([-0.42296103], dtype=float32), u'weight': array([[-0.27337465, -0.14563249]], dtype=float32), u'gradBias': array([ 0.], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "# the input data size is 2*1, the output size is 1*1\n",
    "linear = Linear(2, 1)\n",
    "# print the randomly initialized parameters\n",
    "print linear.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.40507069], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input = np.array([1,-2])\n",
    "# forward to output\n",
    "output = linear.forward(input)\n",
    "print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we backpropagate the error of the predicted output to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createAbsCriterion\n",
      "loss: 0.4050707\n",
      "{u'com.intel.analytics.bigdl.nn.Linear@a835bb1f': {u'gradWeight': array([[-1.,  2.]], dtype=float32), u'bias': array([-0.42296103], dtype=float32), u'weight': array([[-0.27337465, -0.14563249]], dtype=float32), u'gradBias': array([-1.], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "# mean absolute error\n",
    "mae = AbsCriterion()\n",
    "target = np.array([0])\n",
    "\n",
    "loss = mae.forward(output, target)\n",
    "print(\"loss: \" + str(loss))\n",
    "        \n",
    "grad_output = mae.backward(output, target)\n",
    "linear.backward(input, grad_output)\n",
    "\n",
    "print linear.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that the backward pass has computed the gradient of the weights in respect to the loss. Therefore we can update the weights with the gradients using algorithms such as *stochastic gradient descent*. However in practice you **should** use *optimizer.optimize()* to circumvent the details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
