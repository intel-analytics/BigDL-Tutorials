{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will introduce how to build a logistic regression model using BigDL. We use *MNIST* data for experiments in this tutorial. For more information about MNIST, please refer to this [site](http://yann.lecun.com/exdb/mnist/). The first thing we need to do it to import necessary packages and inilialize the engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part aims at preparing for loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import java.nio.ByteBuffer\n",
    "import java.nio.file.{Files, Path, Paths}\n",
    "\n",
    "import com.intel.analytics.bigdl.dataset.ByteRecord\n",
    "import com.intel.analytics.bigdl.utils.File\n",
    "import scopt.OptionParser\n",
    "\n",
    "def load(featureFile: String, labelFile: String): Array[ByteRecord] = {\n",
    "    val featureBuffer = ByteBuffer.wrap(Files.readAllBytes(Paths.get(featureFile)))\n",
    "    val labelBuffer = ByteBuffer.wrap(Files.readAllBytes(Paths.get(labelFile)))\n",
    "    \n",
    "    val labelMagicNumber = labelBuffer.getInt()\n",
    "    require(labelMagicNumber == 2049)\n",
    "    val featureMagicNumber = featureBuffer.getInt()\n",
    "    require(featureMagicNumber == 2051)\n",
    "\n",
    "    val labelCount = labelBuffer.getInt()\n",
    "    val featureCount = featureBuffer.getInt()\n",
    "    require(labelCount == featureCount)\n",
    "\n",
    "    val rowNum = featureBuffer.getInt()\n",
    "    val colNum = featureBuffer.getInt()\n",
    "\n",
    "    val result = new Array[ByteRecord](featureCount)\n",
    "    var i = 0\n",
    "    while (i < featureCount) {\n",
    "      val img = new Array[Byte]((rowNum * colNum))\n",
    "      var y = 0\n",
    "      while (y < rowNum) {\n",
    "        var x = 0\n",
    "        while (x < colNum) {\n",
    "          img(x + y * colNum) = featureBuffer.get()\n",
    "          x += 1\n",
    "        }\n",
    "        y += 1\n",
    "      }\n",
    "      result(i) = ByteRecord(img, labelBuffer.get().toFloat + 1.0f)\n",
    "      i += 1\n",
    "    }\n",
    "\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "import com.intel.analytics.bigdl._\n",
    "import com.intel.analytics.bigdl.utils._\n",
    "import com.intel.analytics.bigdl.dataset.DataSet\n",
    "import com.intel.analytics.bigdl.dataset.image.{BytesToGreyImg, GreyImgNormalizer, GreyImgToBatch, GreyImgToSample}\n",
    "import com.intel.analytics.bigdl.nn.{ClassNLLCriterion, Module}\n",
    "import com.intel.analytics.bigdl.numeric.NumericFloat\n",
    "import com.intel.analytics.bigdl.optim._\n",
    "import com.intel.analytics.bigdl.utils.{Engine, LoggerFilter, T, Table}\n",
    "import com.intel.analytics.bigdl.models.lenet.Utils._\n",
    "import com.intel.analytics.bigdl.nn.{ClassNLLCriterion, Linear, LogSoftMax, Sequential, Reshape}\n",
    "import com.intel.analytics.bigdl.optim.SGD\n",
    "import com.intel.analytics.bigdl.optim.Top1Accuracy\n",
    "import com.intel.analytics.bigdl.tensor._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get and store MNIST for training and testing. You should edit the paths below according to your system settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val trainData = \"../../datasets/mnist/train-images-idx3-ubyte\"\n",
    "val trainLabel = \"../../datasets/mnist/train-labels-idx1-ubyte\"\n",
    "val validationData = \"../../datasets/mnist/t10k-images-idx3-ubyte\"\n",
    "val validationLabel = \"../../datasets/mnist/t10k-labels-idx1-ubyte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Parameters\n",
    "val batchSize = 2048\n",
    "val learningRate = 0.2\n",
    "val maxEpochs = 15\n",
    "\n",
    "//Network Parameters\n",
    "val nInput = 784 //MNIST data input (img shape: 28*28)\n",
    "val nClasses = 10  //MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Engine.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val trainSet = \n",
    "    DataSet.array(load(trainData, trainLabel), sc) -> BytesToGreyImg(28, 28) -> GreyImgNormalizer(trainMean, trainStd) -> GreyImgToBatch(batchSize)\n",
    "val validationSet = \n",
    "    DataSet.array(load(validationData, validationLabel), sc) -> BytesToGreyImg(28, 28) -> GreyImgNormalizer(testMean, testStd) -> GreyImgToBatch(batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential[9857d67b]{\n",
       "  [input -> (1) -> (2) -> (3) -> output]\n",
       "  (1): Reshape[3dec9b34](784)\n",
       "  (2): Linear[c5ea5213](784 -> 10)\n",
       "  (3): LogSoftMax[5b4a673]\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = Sequential().add(Reshape(Array(28 * 28))).add(Linear(nInput, nClasses)).add(LogSoftMax())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.intel.analytics.bigdl.optim.DistriOptimizer@7ed045fb"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val optimizer = Optimizer(model = model, dataset = trainSet, criterion = ClassNLLCriterion[Float]())\n",
    "optimizer.setValidation(trigger = Trigger.everyEpoch, dataset = validationSet, vMethods = Array(new Top1Accuracy[Float], new Top5Accuracy[Float], new Loss[Float]))\n",
    "optimizer.setOptimMethod(new SGD(learningRate=learningRate))\n",
    "optimizer.setEndWhen(Trigger.maxEpoch(maxEpochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't find locality partition for partition 0 Partition locations are (ArrayBuffer(172.168.2.109)) Candidate partition locations are\n",
      "(0,List()).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential[9857d67b]{\n",
       "  [input -> (1) -> (2) -> (3) -> output]\n",
       "  (1): Reshape[3dec9b34](784)\n",
       "  (2): Linear[c5ea5213](784 -> 10)\n",
       "  (3): LogSoftMax[5b4a673]\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainedModel = optimizer.optimize()\n",
    "trainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1Accuracy is Accuracy(correct: 9223, count: 10000, accuracy: 0.9223)\n"
     ]
    }
   ],
   "source": [
    "val rddData = sc.parallelize(load(validationData, validationLabel), batchSize)\n",
    "val transformer = BytesToGreyImg(28, 28) -> GreyImgNormalizer(testMean, testStd) -> GreyImgToSample()\n",
    "val evaluationSet = transformer(rddData)\n",
    "        \n",
    "val result = model.evaluate(evaluationSet, Array(new Top1Accuracy[Float]), Some(batchSize))\n",
    "\n",
    "result.foreach(r => println(s\"${r._2} is ${r._1}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0,3.0,2.0,1.0,7.0,2.0,5.0,10.0,7.0,1.0,1.0,7.0,10.0,1.0,4.0,6.0,10.0,8.0,4.0,6.0\n",
      "8.0,3.0,2.0,1.0,5.0,2.0,5.0,10.0,6.0,10.0,1.0,7.0,10.0,1.0,2.0,6.0,10.0,8.0,4.0,5.0\n"
     ]
    }
   ],
   "source": [
    "val predictions = model.predict(evaluationSet)\n",
    "val preLabels = predictions.take(20).map(_.toTensor.max(1)._2.valueAt(1)).mkString(\",\")\n",
    "val labels = evaluationSet.take(20).map(_.label.valueAt(1)).mkString(\",\")\n",
    "println(preLabels)\n",
    "println(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
