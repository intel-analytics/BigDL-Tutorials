{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with ``BigDL-Keras``\n",
    "\n",
    "Now that we've implemented a whole neural network from scratch, using nothing but `numpy`, let's see how we can make the same model while doing a lot less work. \n",
    "\n",
    "Again, let's import some packages, this time adding ``mxnet.gluon`` to the list of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Dataset\n",
    "\n",
    "Again we'll look at the problem of linear regression and stick with the same synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_examples = 10000\n",
    "input_mean = [0, 0]\n",
    "input_variance = [[1, 0],[0, 1]]\n",
    "noise_mean = 0\n",
    "noise_variance = 0.01\n",
    "\n",
    "def real_fn(X):\n",
    "    return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
    "    \n",
    "X = np.random.multivariate_normal(input_mean, input_variance, num_examples)\n",
    "X /= np.max(X)\n",
    "noise = np.random.normal(noise_mean, noise_variance, num_examples)\n",
    "Y = real_fn(X) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "\n",
    "When we implemented things from scratch, \n",
    "we had to individually allocate parameters \n",
    "and then compose them together as a model. \n",
    "While it's good to know how to do things from scratch, \n",
    "with `bigdl-keras`, we can just compose a network from predefined layers. \n",
    "For a linear model, the appropriate layer is called `Dense`. \n",
    "It's called a *dense* layer because every node in the input \n",
    "is connected to every node in the subsequent layer. \n",
    "That description seems excessive \n",
    "because we only have one (non-input) layer here, \n",
    "and that layer only contains one node!\n",
    "But in subsequent chapters we'll typically work \n",
    "with networks that have multiple outputs, \n",
    "so we might as well start thinking in terms of layers of nodes. \n",
    "Because a linear model consists of just a single `Dense` layer, we can instantiate it with one line.\n",
    "\n",
    "As in [the previous notebook](linear-regression-scratch.ipynb), \n",
    "we have an input dimension of 2 and an output dimension of 1. \n",
    "the most direct way to instantiate a ``Dense`` layer with these dimensions\n",
    "is to specify the number of inputs and the number of outputs. You can pass a name to the dense layer. Let's call it \"Linear\". Also, you can specify whether to add a bias in your linear model. Although the default boolean value for 'bias' is 'True', we just put it explicitly to get a better view of usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createKerasDense\n"
     ]
    }
   ],
   "source": [
    "from bigdl.nn.keras.layer import Dense\n",
    "dense = Dense(1, input_dim = 2, name=\"Linear\", bias=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In BigDL Keras API, we use 'Sequential' as a container to store stacks of layers. Now that we created a dense layer, it's needed to be added into a Sequential constructor to finish defining a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/megaspoon/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createKerasSequential\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.nn.keras.topology.Sequential at 0x7fbd60796c50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bigdl.nn.keras.topology import Sequential\n",
    "linear_model = Sequential()\n",
    "linear_model.add(dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape Inference\n",
    "After creating a model, we can use the `get_input_shape` and `get_output_shape` to\n",
    "access the input or output shape of a model, which is a shape tuple. \n",
    "The first entry is `None` representing the batch dimension. \n",
    "For a model with multiple inputs or outputs, a list of shape tuples will be returned.\n",
    "\n",
    "You can use `flatterned_layers` to know what inside layers are given the unknown model. \n",
    "It will return a list of the contained layers in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 2)\n",
      "(None, 1)\n",
      "Dense[Linear]\n"
     ]
    }
   ],
   "source": [
    "print(linear_model.get_input_shape())\n",
    "print(linear_model.get_output_shape())\n",
    "for layer in linear_model.flattened_layers(): print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Training\n",
    "You might have noticed that it was a bit more concise to express our model in BigDL-Keras. \n",
    "For example, we didn't have to individually allocate parameters, define our loss function,\n",
    "implement stochastic gradient descent or create a validation method.\n",
    "Simply, we use `compile` to set all these parameters in one station. \n",
    "The benefits of relying on BigDL's abstractions will grow substantially \n",
    "once we start working with much more complex models. \n",
    "\n",
    "In this case, we specify MSE(mean_squared_error) as our loss, \n",
    "SGD(stochastic gradient descent) as our optimizer \n",
    "and Top1Accuracy as our validation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createDefault\n",
      "creating: createSGD\n",
      "creating: createMSECriterion\n",
      "creating: createTop1Accuracy\n"
     ]
    }
   ],
   "source": [
    "linear_model.compile(loss='mse',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Training\n",
    "With all these pieces together, we can start our training in a much easier way than that in scratch. The Linear Regression model training process will be like for each epoch, \n",
    "- grabbing mini-batch of the inputs and feeding them into the model\n",
    "- compare with the corresponding ground-truth labels to compute loss and graident \n",
    "- Use the graident to update the model weight and biases\n",
    "\n",
    "You don't need to initialize the weights and biases of the model since `fit` method will automatically do it for you. \n",
    "You only need to specify the batch size and number of epochs. If additional validation dataset is available, you can pass \n",
    "it to `fit` as well. We will try validation in our later chapters so we assign `validation_data=None` for now. The `distributed=True` will indicate the training will be executed in a distributed mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.fit(X, Y, batch_size = 8, nb_epoch=10, validation_data=None, distributed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find in the terminal, the training process utilize Spark's high-speed distributed computing to accelerate itself. Normlly, the loss decreases after each iteration. You can explore what will change if batch size and epoch number is tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "As you can see, even for a simple example like linear regression, ``BigDL-Keras`` API can help you to write quick and clean code. Next, we'll repeat this exercise for multi-layer perceptrons, extending these lessons to deep neural networks and (comparatively) real datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[Binary classification with logistic regression](../supervised-learning/logistic-regression-bigdl-keras.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
