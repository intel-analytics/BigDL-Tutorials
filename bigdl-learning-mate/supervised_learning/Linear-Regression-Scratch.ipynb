{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression from scratch\n",
    "\n",
    "Powerful ML libraries can eliminate repetitive work, but if you rely too much on abstractions, you might never learn how neural networks really work under the hood. So for this first example, let's get our hands dirty and build everything from scratch, relying only on Numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "\n",
    "To get our feet wet, we'll start off by looking at the problem of regression.\n",
    "This is the task of predicting a *real valued target* $y$ given a data point $x$.\n",
    "In linear regression, the simplest and still perhaps the most useful approach,\n",
    "we assume that prediction can be expressed as a *linear* combination of the input features \n",
    "(thus giving the name *linear* regression):\n",
    "\n",
    "$$\\hat{y} = w_1 \\cdot x_1 + ... + w_d \\cdot x_d + b$$\n",
    "\n",
    "\n",
    "Given a collection of data points $X$, and corresponding target values $\\boldsymbol{y}$, \n",
    "we'll try to find the *weight* vector $\\boldsymbol{w}$ and bias term $b$ \n",
    "(also called an *offset* or *intercept*)\n",
    "that approximately associate data points $\\boldsymbol{x}_i$ with their corresponding labels $\\boldsymbol{y}_i$. \n",
    "Using slightly more advanced math notation, we can express the predictions $\\boldsymbol{\\hat{y}}$\n",
    "corresponding to a collection of datapoints $X$ via the matrix-vector product:\n",
    "\n",
    "$$\\boldsymbol{\\hat{y}} = X \\boldsymbol{w} + b$$\n",
    "\n",
    "\n",
    "Before we can get going, we will need two more things \n",
    "\n",
    "* Some way to measure the quality of the current model  \n",
    "* Some way to manipulate the model to improve its quality\n",
    "\n",
    "\n",
    "### Square loss\n",
    "In order to say whether we've done a good job, \n",
    "we need some way to measure the quality of a model. \n",
    "Generally, we will define a *loss function*\n",
    "that says *how far* are our predictions from the correct answers.\n",
    "For the classical case of linear regression, \n",
    "we usually focus on the squared error.\n",
    "Specifically, our loss will be the sum, over all examples, of the squared error $(y_i-\\hat{y})^2)$ on each:\n",
    "\n",
    "$$\\ell(y, \\hat{y}) = \\sum_{i=1}^n (\\hat{y}_i-y_i)^2.$$\n",
    "\n",
    "\n",
    "For one-dimensional data, we can easily visualize the relationship between our single feature and the target variable. It's also easy to visualize a linear predictor and it's error on each example. \n",
    "Note that squared loss *heavily penalizes outliers*. For the visualized predictor below, the lone outlier would contribute most of the loss.\n",
    "\n",
    "![](../img/linear-regression.png)\n",
    "\n",
    "\n",
    "### Manipulating the model\n",
    "\n",
    "For us to minimize the error,\n",
    "we need some mechanism to alter the model.\n",
    "We do this by choosing values of the *parameters*\n",
    "$\\boldsymbol{w}$ and $b$.\n",
    "This is the only job of the learning algorithm.\n",
    "Take training data ($X$, $y$) and the functional form of the model $\\hat{y} = X\\boldsymbol{w} + b$.\n",
    "Learning then consists of choosing the best possible $\\boldsymbol{w}$ and $b$ based on the available evidence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Historical note\n",
    "\n",
    "You might reasonably point out that linear regression is a classical statistical model.\n",
    "[According to Wikipedia](https://en.wikipedia.org/wiki/Regression_analysis#History), \n",
    "Legendre first developed the method of least squares regression in 1805,\n",
    "which was shortly thereafter rediscovered by Gauss in 1809. \n",
    "Presumably, Legendre, who had Tweeted about the paper several times,\n",
    "was peeved that Gauss failed to cite his arXiv preprint. \n",
    "\n",
    "![Legendre](../img/legendre.jpeg)\n",
    "\n",
    "\n",
    "Matters of provenance aside, you might wonder - if Legendre and Gauss \n",
    "worked on linear regression, does that mean there were the original deep learning researchers?\n",
    "And if linear regression doesn't wholly belong to deep learning, \n",
    "then why are we presenting a linear model \n",
    "as the first example in a tutorial series on neural networks? \n",
    "Well it turns out that we can express linear regression \n",
    "as the simplest possible (useful) neural network. \n",
    "A neural network is just a collection of nodes (aka neurons) connected by directed edges. \n",
    "In most networks, we arrange the nodes into layers with each feeding its output into the layer above. \n",
    "To calculate the value of any node, we first perform a weighted sum of the inputs (according to weights ``w``) \n",
    "and then apply an *activation function*. \n",
    "For linear regression, we only have two layers, one corresponding to the input (depicted in orange) \n",
    "and a one-node layer (depicted in green) correspnding to the ouput.\n",
    "For the output node the activation function is just the identity function.\n",
    "\n",
    "![](../img/onelayer.png)\n",
    "\n",
    "While you certainly don't have to view linear regression through the lens of deep learning, \n",
    "you can (and we will!).\n",
    "To ground the concepts that we just discussed in code, \n",
    "let's actually code up a neural network for linear regression from scratch.\n",
    "\n",
    "\n",
    "To get going, we will generate a simple synthetic dataset by sampling random data points ``X[i]`` and corresponding labels ``y[i]`` in the following manner. Our inputs will each be sampled from a random normal distribution with mean $0$ and variance $1$. Our features will be independent. Another way of saying this is that they will have diagonal covariance.  The labels will be generated accoding to the *true* labeling function `y[i] = 2 * X[i][0]- 3.4 * X[i][1] + 4.2 + noise` where the noise is drawn from a random gaussian with mean ``0`` and variance ``.01``. We could express the labeling function in mathematical notation as:\n",
    "$$y = X \\cdot w + b + \\eta, \\quad \\text{for } \\eta \\sim \\mathcal{N}(0,\\sigma^2)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_outputs = 1\n",
    "num_examples = 10000\n",
    "input_mean = [0, 0]\n",
    "input_variance = [[1, 0],[0, 1]]\n",
    "noise_mean = 0\n",
    "noise_variance = 0.01\n",
    "\n",
    "def real_fn(X):\n",
    "    return 2 * X[:, 0] - 3.4 * X[:, 1] + 4.2\n",
    "    \n",
    "X = np.random.multivariate_normal(input_mean, input_variance, num_examples)\n",
    "X /= np.max(X)\n",
    "noise = np.random.normal(noise_mean, noise_variance, num_examples)\n",
    "y = real_fn(X) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row in ``X`` consists of a 2-dimensional data point and that each row in ``Y`` consists of a 1-dimensional target value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.24470042  0.22835131]\n",
      "3.91519102645\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a linear combination with the (known) optimal parameters \n",
    "produces a prediction that is indeed close to the target value where the random noise brings the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.913006397\n"
     ]
    }
   ],
   "source": [
    "print(2 * X[0, 0] - 3.4 * X[0, 1] + 4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the correspondence between our second feature (``X[:, 1]``) and the target values ``Y`` by generating a scatter plot with the Python plotting package ``matplotlib``. Make sure that ``matplotlib`` is installed. Otherwise, you may install it by running ``pip2 install matplotlib`` (for Python 2) or ``pip3 install matplotlib`` (for Python 3) on your command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/NK27GtUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0a9c5f790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterators\n",
    "\n",
    "Once we start working with neural networks, we're going to need to iterate through our data points quickly. In BigDL, we fully utilize the convenient and friendly funcationalities of Spark's RDD, allowing users to transform their original ``nd_array``-type features-label pairs into Spark RDDs by using a simple function ``to_sample_rdd``. The ``to_sample_rdd`` has another ``num_Slices`` parameter which indicates the number of partitions you want for the result RDD. With specification, its default value equals the number of CPU cores currently used. You can use `sc.defaultParallelism` to check it programmatically or just simply find it in the start script for the `spark-submit` job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bigdl.util.common import *\n",
    "rdd = to_sample_rdd(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sc.defaultParallelism)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As [Spark's rdd programming guide](https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#parallelized-collections) suggests, typically you want 2-4 partitions for each CPU in your cluster. So, let's reset the partition number to 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = to_sample_rdd(X, y, 8)\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is transformed into RDD, you can access the powerful methods of RDD to inspect more easily on your data. You can even customize your methods and chain them together to unleash more information about the training inputs. To give you a hint to better understand the usage, the following code will print the features and labels pair by pair under the new format in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printFeatureLabelPairs(record):\n",
    "        print('features: '+str(record.features)+' label: '+str(record.label))\n",
    "rdd.foreach(printFeatureLabelPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "\n",
    "Now let's allocate some memory for our parameters and set their initial values randomly from the Guassian ditribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.random.multivariate_normal(input_mean, input_variance, num_examples)\n",
    "b = np.random.normal(0, 1, num_examples)\n",
    "params = [w, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the succeeding cells, we're going to update these parameters to better fit our data. This will involve taking the gradient (a multi-dimensional derivative) of some *loss function* with respect to the parameters. We'll update each parameter in the direction that reduces the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "Next we'll want to define our model. In this case, we'll be working with linear models, the simplest possible *useful* neural network. To calculate the output of the linear model, we simply multiply a given input with the model's weights (``w``), and add the offset ``b``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return w.dot(X.transpose()) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that was easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Train a model means making it better and better over the course of a period of training. But in order for this goal to make any sense at all, we first need to define what *better* means in the first place. In this case, we'll use the squared distance between our prediction and the true value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_loss(yhat, y): \n",
    "    return np.mean((yhat - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "It turns out that linear regression actually has a closed-form solution. However, most interesting models that we'll care about cannot be solved analytically. So we'll solve this problem by stochastic gradient descent. At each step, we'll estimate the gradient of the loss with respect to our weights. Then, we'll update our parameters a small amount in the direction that reduces the loss. The size of the step is determined by the *learning rate* ``lr``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gradient(w, x, y):\n",
    "    y_estimate = net(x)\n",
    "    error = y - y_estimate\n",
    "    gradient = -(1.0/len(x)) * error.dot(x)\n",
    "    loss= square_loss(y, y_estimate)\n",
    "    return gradient, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged.\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "tolerance = 1e-3\n",
    "\n",
    "# Perform Gradient Descent\n",
    "epochs = 1\n",
    "while True:\n",
    "    gradient, loss = get_gradient(w, X, y)\n",
    "    new_w = w - lr * gradient\n",
    "    \n",
    "    # Stopping Condition\n",
    "    if np.sum(abs(new_w - w)) < tolerance:\n",
    "        print(\"Converged.\")\n",
    "        break\n",
    "    \n",
    "    # Print error every 50 iterations\n",
    "    if epochs % 3 == 0:\n",
    "        print(\"Iteration: %d - Loss: %.4f\" %(epochs, loss))\n",
    "    \n",
    "    epochs += 1\n",
    "    w = new_w\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion \n",
    "\n",
    "You've seen that using just Numpy, we can build statistical models from scratch. In the following tutorials, we'll build on this foundation, introducing the basic ideas behind modern neural networks and demonstrating the powerful abstractions in BigDL-Keras package for building complex models with little code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "[Linear regression with BigDL-Keras](../supervised_learning/Linear-Regression-With-BigDL-Keras.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
