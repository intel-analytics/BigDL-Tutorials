{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification with logistic regression\n",
    "\n",
    "Over the last two tutorials we worked through how to implement a linear regression model,\n",
    "both [*from scratch*](Linear-Regression-Scratch.ipynb)\n",
    "and [using BigDL-Keras](Linear-Regression-With-BigDL-Keras.ipynb) to automate most of the repetitive work \n",
    "like allocating and initializing parameters, defining loss functions, and implementing optimizers.\n",
    "\n",
    "Regression is the hammer we reach for when we want to answer *how much?* or *how many?* questions.\n",
    "If you want to predict the number of dollars (the *price*) at which a house will be sold,\n",
    "or the number of wins a baseball team might have, \n",
    "or the number of days that a patient will remain hospitalized before being discharged,\n",
    "then you're probably looking for a regression model.\n",
    "\n",
    "Based on our experience, in industry, we're more often interested in making categorical assignments.\n",
    "*Does this email belong in the spam folder or the inbox*?\n",
    "*How likely is this custromer to sign up for subscription service?*\n",
    "When we're interested in either assigning datapoints to categories\n",
    "or assessing the *probability* that a category applies,\n",
    "we call this task *classification*. \n",
    "\n",
    "The simplest kind of classification problem is *binary classification*,\n",
    "when there are only two categories,\n",
    "so let's start there. \n",
    "Let's call our two categories the positive class $y_i=1$ and the negative class $y_i = 0$.\n",
    "Even with just two categories, and even confining ourselves to linear models, \n",
    "there are many ways we might approach the problem. \n",
    "For example, we might try to draw a line that best separates the points.\n",
    "\n",
    "![](../img/linear-separator.png)\n",
    "\n",
    "A whole family of algorithms called support vector machines pursue this approach.\n",
    "The main idea here is choose a line that maximizes the margin to the closest data points on either side of the decision boundary. \n",
    "In these approaches, only the points closest to the decision boundary (the support vectors) \n",
    "actually influence the choice of the linear separator.\n",
    "\n",
    "With neural networks, we usually approach the problem differently. \n",
    "Instead of just trying to separate the points,\n",
    "we train a probabilistic classifier which estimates,\n",
    "for each data point, the conditional probability that it belongs to the positive class. \n",
    "\n",
    "Recall that in linear regression, we made predictions of the form\n",
    "\n",
    "$$ \\hat{y} = \\boldsymbol{w}^T \\boldsymbol{x} + b. $$\n",
    "\n",
    "We are interested in asking the question *\"what is the probability that example $x$ belongs to the positive class?\"*\n",
    "A regular linear model is a poor choice here because it can output values greater than $1$ or less than $0$.\n",
    "To coerce reasonable answers from our model, \n",
    "we're going to modify it slightly,\n",
    "by running the linear function through a sigmoid activation function $\\sigma$:\n",
    "\n",
    "$$ \\hat{y} =\\sigma(\\boldsymbol{w}^T \\boldsymbol{x} + b). $$\n",
    "\n",
    "The sigmoid function $\\sigma$, sometimes called a squashing function or a *logistic* function - thus the name logistic regression - maps a real-valued input to the range 0 to 1.\n",
    "Specifically, it has the functional form:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Let's get our imports out of the way and visualize the logistic function using `BigDL` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc73a90a290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "    \n",
    "x = np.linspace(-2*np.pi, 2*np.pi, 100)    \n",
    "y = logistic(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the sigmoid outputs a value between $0$ and $1$,\n",
    "it's more reasonable to think of it as a probability.\n",
    "Note that an input of $0$ gives a value of $.5$. \n",
    "So in the common case, where we want to predict positive whenever the probability is greater than $.5$\n",
    "and negative whenever the probability is less than $.5$,\n",
    "we can just look at the sign of $\\boldsymbol{w}^T \\boldsymbol{x} + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary cross-entropy loss\n",
    "\n",
    "Now that we've got a model that outputs probabilities,\n",
    "we need to choose a loss function.\n",
    "When we wanted to predict *how much* we used squared error $y-\\hat{y}^2$,\n",
    "as our measure our model's performance. \n",
    "\n",
    "Since now we're thinking about outputing probabilities,\n",
    "one natural objective is to say that we should choose the weights \n",
    "that give the actual labels in the training data the highest probability.\n",
    "\n",
    "$$\\max_{\\theta} P_{\\theta}( (y_1, ..., y_n) | \\boldsymbol{x}_1,...,\\boldsymbol{x}_n )$$\n",
    "\n",
    "Because each example is independent of the others, and each label depends only on the features of the corresponding examples, we can rewrite the above as\n",
    "\n",
    "$$\\max_{\\theta} P_{\\theta}(y_1|\\boldsymbol{x}_1)P_{\\theta}(y_2|\\boldsymbol{x}_2) ... P(y_n|\\boldsymbol{x}_n)$$\n",
    "\n",
    "\n",
    "\n",
    "This function is a product over the examples, but in general, because we want to train by stochastic gradient descent, it's a lot easier to work with a loss function that breaks down as a sum over the training examples. \n",
    "\n",
    "$$ \\max_{\\theta} \\log P_{\\theta}(y_1|\\boldsymbol{x}_1) + ... + \\log P(y_n|\\boldsymbol{x}_n)$$\n",
    "\n",
    "Because we typically express our objective as a *loss* we can just flip the sign, giving us the *negative log probability:*\n",
    "\n",
    "$$  \\min_{\\theta} \\left(- \\sum_{i=1}^n \\log P_{\\theta}(y_i|\\boldsymbol{x}_i)\\right)$$\n",
    "\n",
    "If we interpret $\\hat{y_i}$ as the probability that the $i$-th example belongs to the positive class (i.e $y_i=1$),\n",
    "then $1 - \\hat{y_i}$ is the probability that the $i$-th example belongs to the negative class (i.e $y_i=0$). This is equivalent to saying\n",
    "\n",
    "$$ P_{\\theta}(y_i|\\boldsymbol{x}_i) = \\begin{cases}\n",
    "    \\hat{y}_i, & \\text{if } y_i = 1\\\\\n",
    "    1-\\hat{y}_i, & \\text{if } y_i = 0\n",
    "\\end{cases} $$\n",
    "\n",
    "which can be written in a more compact form\n",
    "\n",
    "$$ P_{\\theta}(y_i|\\boldsymbol{x}_i) = \\hat{y}_i^{y_i}(1-\\hat{y}_i)^{1-y_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can express our learning objective as:\n",
    "\n",
    "$$ \\ell (\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) =  - \\sum_{i=1}^n y_i \\log \\hat{y}_i + (1-y_i) \\log (1-\\hat{y}_i).$$\n",
    "\n",
    "\n",
    "If you're learning machine learning for the first time, that might have been too much information too quickly. \n",
    "Let's take a look at this loss function and break down what's going on more slowly. \n",
    "The loss function consists of two terms, $y_i \\log \\hat{y}_i$ and $(1-y_i) \\log (1-\\hat{y}_i)$.\n",
    "Because $y_i$ only takes values $0$ or $1$, for a given data point, one of these terms disappears. \n",
    "When $y_i$ is $1$, this loss says that we should maximize $\\log \\hat{y}_i$, giving higher probability to the *correct* answer. \n",
    "When $y_i$ is $0$, this loss function takes value $\\log (1-\\hat{y}_i)$. That says that we should maximize the value $1-\\hat{y}$ which we already know is the probability assigned to $\\boldsymbol{x}_i$ belonging to the negative class.\n",
    "\n",
    "\n",
    "Note that this loss function is commonly called *log loss* and is also commonly referred to as *binary cross entropy*. It is a special case of negative log likelihood. And it is a special case of cross-entropy, which can apply to the multi-class ($>2$) setting.\n",
    "\n",
    " \n",
    "While for linear regression, we demonstrated a completely different implementation *from scratch* and *with `gluon`*, here we're going to demonstrate how we can mix and match the two.  We'll use `gluon` for our modeling, but we'll write our loss function from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "As usual, we'll want to work out these concepts using a real dataset. This time around, we'll use the *Adult* dataset taken from the [UCI repository](http://archive.ics.uci.edu/ml/datasets/). \n",
    "The dataset was constructed by Barry Becker from 1994 census data.\n",
    "In its original form, the dataset contained $14$ features, including age, education, occupation, sex, native-country, among others. \n",
    "In this version, [hosted by National Taiwan University](https://www.csie.ntu.edu.tw/%7Ecjlin/libsvmtools/datasets/binary.html),\n",
    "the data have been re-processed to $123$ binary features each representing quantiles among the original features.\n",
    "The label is a binary indicator indicating whether the person corresponding to each row made more ($y_i = 1$) or less ($y_i = 0$) than $50,000 of income in 1994.\n",
    "The dataset we're working with contains 30,956 training examples and 1,605 examples set aside for testing.\n",
    "We can read the datasets into main memory like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
