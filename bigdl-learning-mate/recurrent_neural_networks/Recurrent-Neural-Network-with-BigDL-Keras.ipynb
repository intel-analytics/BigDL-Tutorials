{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks with ``BigDL``\n",
    "\n",
    "\n",
    "With BigDL, now we can train the recurrent neural networks (RNNs) more neatly, such as the long short-term memory (LSTM) and the gated recurrent unit (GRU). To demonstrate the end-to-end RNN training and prediction pipeline, we take a classic problem in language modeling as a case study. Specifically, we will show how to predict the distribution of the next word given a sequence of previous words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "To begin with, we need to make the following necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes for indexing words of the input document\n",
    "\n",
    "In a language modeling problem, we define the following classes to facilitate the routine procedures for loading document data. In the following, the ``Dictionary`` class is for word indexing: words in the documents can be converted from the string format to the integer format. \n",
    "\n",
    "In this example, we use consecutive integers to index words of the input document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``Dictionary`` class is used by the ``Corpus`` class to index the words of the input document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r') as f:\n",
    "            ids = np.zeros((tokens,), dtype='int32')\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data as batches\n",
    "\n",
    "We load the document data by leveraging the aforementioned ``Corpus`` class. \n",
    "\n",
    "To speed up the subsequent data flow in the RNN model, we pre-process the loaded data as batches. This procedure is defined in the following ``batchify`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"../data/nlp/ptb.\"\n",
    "\n",
    "corpus = Corpus(data_path)\n",
    "\n",
    "def batchify(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, 1, nbatch)).T\n",
    "    return data\n",
    "\n",
    "batch_size = 32\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide an exposition of different RNN models\n",
    "We can make different RNN models available with the following single ``RNNModel`` class.\n",
    "\n",
    "Users can select their preferred RNN model or compare different RNN models by configuring the argument of the constructor of ``RNNModel``. We will show an example following the definition of the ``RNNModel`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bigdl.nn.keras.layer import *\n",
    "from bigdl.nn.keras.topology import Sequential\n",
    "\n",
    "class RNNModel():\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"   \n",
    "    #batch_size was defined 32\n",
    "    def __init__(self, mode, vocab_size, num_hidden, arg_input_shape = (1, batch_size), dropout=0.5):\n",
    "            self.model = Sequential()\n",
    "            if mode == 'rnn_relu':\n",
    "                self.model.add(SimpleRNN(num_hidden, activation = \"relu\", input_shape = arg_input_shape))\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.model.add(SimpleRNN(num_hidden, input_shape = arg_input_shape))\n",
    "            elif mode == 'lstm':\n",
    "                self.model.add(LSTM(num_hidden, input_shape = arg_input_shape))\n",
    "            elif mode == 'gru':\n",
    "                self.model.add(GRU(num_hidden, input_shape = arg_input_shape))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)\n",
    "            # self.model.add(Dropout(dropout))\n",
    "            self.decoder = Dense(vocab_size, activation = \"tanh\")\n",
    "            self.model.add(self.decoder)\n",
    "            self.num_hidden = num_hidden\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "We go on to build the model, initialize model parameters, and configure the optimization algorithms for training the RNN model.\n",
    "\n",
    "For demonstration, LSTM is the chosen RNN model type. For other RNN options, one can replace the 'lstm' string to 'rnn_relu', 'rnn_tanh', or 'gru'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createKerasSequential\n",
      "creating: createKerasLSTM\n",
      "creating: createKerasDense\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model_type = 'lstm'\n",
    "num_hid = 100\n",
    "LSTM = RNNModel(model_type, ntokens, num_hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 32)\n",
      "(None, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(LSTM.model.get_input_shape())\n",
    "print(LSTM.model.get_output_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and evaluate on validation and testing data sets\n",
    "\n",
    "Now we can define functions for training and evaluating the model. The following are two helper functions that will be used during model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createCrossEntropyCriterion\n",
      "creating: createDefault\n",
      "creating: createSGD\n",
      "creating: createTop1Accuracy\n"
     ]
    }
   ],
   "source": [
    "from bigdl.nn.criterion import *\n",
    "\n",
    "LSTM.model.compile(optimizer='sgd', loss=CrossEntropyCriterion(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM.model.fit(train_data, corpus.train[:len(train_data)], batch_size=8, nb_epoch=1,\n",
    "validation_data=(val_data[:500], corpus.valid[:500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the RNN model training is based on maximization likelihood of observations. For evaluation purposes, we have used the following two measures:\n",
    "\n",
    "* Loss: the loss function is defined as the average negative log likelihood of the target words (ground truth) under prediction: $$\\text{loss} = -\\frac{1}{N} \\sum_{i = 1}^N \\text{log} \\  p_{\\text{target}_i}, $$ where $N$ is the number of predictions and $p_{\\text{target}_i}$ the predicted likelihood of the $i$-th target word.\n",
    "\n",
    "* Perplexity: the average per-word perplexity is $\\text{exp}(\\text{loss})$.\n",
    "\n",
    "To orient the reader using concrete examples, let us illustrate the idea of the perplexity measure as follows.\n",
    "\n",
    "* Consider the perfect scenario where the model always predicts the likelihood of the target word as 1. In this case, for every $i$ we have $p_{\\text{target}_i} = 1$. As a result, the perplexity of the perfect model is 1. \n",
    "\n",
    "* Consider a baseline scenario where the model always predicts the likelihood of the target word randomly at uniform among the given word set $W$. In this case, for every $i$ we have $p_{\\text{target}_i} = 1 / |W|$. As a result, the perplexity of a uniformly random prediction model is always $|W|$. \n",
    "\n",
    "* Consider the worst-case scenario where the model always predicts the likelihood of the target word as 0. In this case, for every $i$ we have $p_{\\text{target}_i} = 0$. As a result, the perplexity of the worst model is positive infinity. \n",
    "\n",
    "\n",
    "Therefore, a model with a lower perplexity that is closer to 1 is generally more effective. Any effective model has to achieve a perplexity lower than the cardinality of the target set.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
