{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest possible way to create a deep neural network is to connect the node in each layer with the node in the subsequent layer. We can say all the layers in the network structure are fully connected. The model is called Multi-Layer Perceptron which can be graphically represented as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/multilayer-perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this can require a lot of parameters! If our input were a 256x256 color image (still quite small for a photograph), and our network had 1,000 nodes in the first hidden layer, then our first weight matrix would require (256x256x3)x1000 parameters. That’s nearly 200 million. Moreover the hidden layer would ignore all the spatial structure in the input image even though we know the local structure represents a powerful source of prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks incorporate convolutional layers. These layers associate each of their nodes with a small window, called a receptive field, in the previous layer, instead of connecting to the full layer. This allows us to first learn local features via transformations that are applied in the same way for the top right corner as for the bottom left. Then we collect all this local information to predict global qualities of the image (like whether or not it depicts a dog)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/depthcol.jpeg)\n",
    "<pre>         (Image credit: Stanford cs231n <a>http://cs231n.github.io/assets/cnn/depthcol.jpeg)</a></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, there are two new concepts you need to grok here. First, we’ll be introducting convolutional layers. Second, we’ll be interleaving them with pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node in convolutional layer is associated with a 3D block (height x width x channel) in the input tensor. Moreover, the convolutional layer itself has multiple output channels. So the layer is parameterized by a 4 dimensional weight tensor, commonly called a convolutional kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tensor is produced by sliding the kernel across the input image skipping locations according to a pre-defined stride (but we’ll just assume that to be 1 in this tutorial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of convolutional layers, pooling layers, fully connected layers and normalization layers. Let's see how succinctly we can use BigDL to express a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from bigdl.nn.keras.topology import Sequential\n",
    "from bigdl.nn.keras.layer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BigDL, we use Convolutiona2D() to apply a 2D convolution over an input image composed of several input planes. This function takes a few important parameters: number of convolutional filters(```nbFilter```), number of rows in the convolutional kernal(```nbRow```), number of columns in the convolutional kernal(```nbCol```), string representation of the activation function(```activation```) and the shape of the input layer(```input_shape```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createKerasSequential\n",
      "creating: createKerasConvolution2D\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(20, 3, 3, activation=\"relu\", input_shape=(1, 28, 28)))\n",
    "input = np.random.random([2, 1, 28, 28])\n",
    "output = model.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shape. The number of examples (64) remains unchanged. The number of channels (also called filters) has increased to 20. And because the (3,3) kernel can only be applied in 26 different heights and widths (without the kernel busting over the image border), our output is 26,26. There are some weird padding tricks we can use when we want the input and output to have the same height and width dimensions, but we won’t get into that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other new component of this model is the pooling layer. Pooling gives us a way to downsample in the spatial dimensions. Early convnets typically used average pooling, but max pooling tends to give better results. The default value for downscaling the weights vertically and horizontally is (2, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createKerasSequential\n",
      "creating: createKerasMaxPooling2D\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(MaxPooling2D(input_shape = (1, 28, 28)))\n",
    "input = np.random.random([2, 1, 28, 28])\n",
    "output = model.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the batch and channel components of the shape are unchanged but that the height and width have been downsampled from (26,26) to (13,13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It flattens the input without affecting the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createKerasSequential\n",
      "creating: createKerasFlatten\n",
      "[[[[ 0.68587314  0.8255428   0.85463511]\n",
      "   [ 0.44956081  0.07858544  0.15813265]]\n",
      "\n",
      "  [[ 0.57645196  0.83081943  0.13454646]\n",
      "   [ 0.75277687  0.3152822   0.78203292]]]\n",
      "\n",
      "\n",
      " [[[ 0.42641051  0.3740926   0.03601874]\n",
      "   [ 0.38221221  0.68646959  0.55002778]]\n",
      "\n",
      "  [[ 0.22202366  0.88417323  0.01115601]\n",
      "   [ 0.71851614  0.65895761  0.78885971]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.68587315,  0.82554281,  0.85463512,  0.44956082,  0.07858545,\n",
       "         0.15813264,  0.57645196,  0.83081943,  0.13454646,  0.75277686,\n",
       "         0.3152822 ,  0.78203291],\n",
       "       [ 0.42641053,  0.37409261,  0.03601874,  0.38221222,  0.68646961,\n",
       "         0.55002779,  0.22202367,  0.88417321,  0.01115601,  0.71851611,\n",
       "         0.6589576 ,  0.78885972]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(2, 2, 3)))\n",
    "input = np.random.random([2, 2, 2, 3])\n",
    "output = model.forward(input)\n",
    "print(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve A Practical Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the previous knowledge to solve the classical problem \"Handwritten Digit Classfication\" using Convolutional Neural Network with BigDL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get MINIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Extracting', 'datasets/mnist/train-images-idx3-ubyte.gz')\n",
      "('Extracting', 'datasets/mnist/train-labels-idx1-ubyte.gz')\n",
      "('Extracting', 'datasets/mnist/t10k-images-idx3-ubyte.gz')\n",
      "('Extracting', 'datasets/mnist/t10k-labels-idx1-ubyte.gz')\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dataset import mnist\n",
    "from bigdl.util.common import *\n",
    "\n",
    "mnist_path = \"datasets/mnist\"\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data(mnist_path)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to define our model．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createKerasSequential\n",
      "creating: createKerasReshape\n",
      "creating: createKerasConvolution2D\n",
      "creating: createKerasMaxPooling2D\n",
      "creating: createKerasConvolution2D\n",
      "creating: createKerasMaxPooling2D\n",
      "creating: createKerasFlatten\n",
      "creating: createKerasDense\n",
      "creating: createKerasDense\n",
      "(None, 28, 28, 1)\n",
      "(None, 10)\n"
     ]
    }
   ],
   "source": [
    "num_fc = 512\n",
    "num_outputs = 10\n",
    "model = Sequential()\n",
    "model.add(Reshape((1, 28, 28), input_shape=(28, 28, 1)))\n",
    "model.add(Convolution2D(20, 3, 3, activation=\"relu\", input_shape=(1, 28, 28)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Convolution2D(50, 3, 3, activation=\"relu\", name=\"conv2_5x5\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_fc, activation=\"relu\", name=\"fc1\"))\n",
    "model.add(Dense(num_outputs, activation=\"softmax\", name=\"fc2\"))\n",
    "\n",
    "print(model.get_input_shape())\n",
    "print(model.get_output_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createDefault\n",
      "creating: createSGD\n",
      "creating: createClassNLLCriterion\n",
      "creating: createTop1Accuracy\n"
     ]
    }
   ],
   "source": [
    "from bigdl.nn.criterion import *\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: See the loss and accuracy in the terminal. We will provide performance visualization in later topics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=8, nb_epoch=1,\n",
    "validation_data=(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
